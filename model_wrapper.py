
import sys
from threading import Thread
import re

from collections import defaultdict
import itertools

from unsloth import FastLanguageModel
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, TextIteratorStreamer
import torch


from constants import PROMPT_TEMPLATE, alpaca_prompt, available_models


class ModelInference:
    def __init__(self, model_path="outputs/checkpoint-7000", max_seq_length=4096, load_in_4bit=True):
        self.is_unsloth = "outputs" in model_path.lower()
        if "llama" in model_path.lower():
            model_path = "unsloth/Meta-Llama-3.1-8B-bnb-4bit" #"unsloth/Hermes-3-Llama-3.1-8B"#
        if "phi" in model_path.lower():
            model_path = "unsloth/Phi-3-mini-4k-instruct"
        self.prompt_template = None
        if self.is_unsloth:
            print(f"Loading fine-tuned model {model_path}")
            self.prompt_template = PROMPT_TEMPLATE
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_path,
                max_seq_length=max_seq_length,
                load_in_4bit=load_in_4bit,
            )
            FastLanguageModel.for_inference(self.model)
        else:
            print(f"Loading {model_path} model")
            self.prompt_template = alpaca_prompt
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_path,
                max_seq_length=4096,
                dtype=torch.float16,
                load_in_4bit=True
            )
            FastLanguageModel.for_inference(self.model) 
        self.model_path = model_path
        for key, val in available_models.items():
            if self.model_path == val:
                self.model_name = key
                break
        self.last_response = ""
        self.streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)
        self.cache = defaultdict(lambda : None)
        torch.cuda.empty_cache()
    
    def replace_spaces(self, text):
        def replacer(match):
            return ' ' * len(match.group())
        pattern = r' {2,}'
        return re.sub(pattern, replacer, text)
    
    def empty_cache(self):
        self.cache = defaultdict(lambda : None)

    def generate(self, input_text, max_new_tokens=4096, top_p=0.9):
        if self.prompt_template:
            input_text = self.prompt_template.format(input_text, "")
        self.tokenizer.pad_token = self.tokenizer.eos_token
        inputs = self.tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=self.model.config.max_position_embeddings)
        input_ids = inputs.input_ids.to(self.model.device)
        with torch.no_grad():
            output = self.model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                top_p=top_p,
                repetition_penalty=1.0,
                use_cache=True,
                streamer=self.streamer
            )    
        return self.tokenizer.decode(output[0], skip_special_tokens=False)


    def generate_stream(self, input_text, max_new_tokens=4096, top_p=0.9):

        if self.prompt_template:
            input_text = self.prompt_template.format(input_text, "")

        if self.cache[input_text] is not None:
            print("Using cached values..")
            for i in self.cache[input_text]:
                yield i
            return
        else:
            self.cache[input_text] = ""

        inputs = self.tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=self.model.config.max_position_embeddings)
        self.last_response = ""        
        input_ids = inputs.input_ids.to(self.model.device)
        kwargs = {
                "input_ids": input_ids,
                "max_new_tokens": max_new_tokens,
                "top_p":top_p,
                "repetition_penalty":1.1,
                "use_cache":True,
                "streamer": self.streamer
        }

        self.gen_thread = Thread(target=self.model.generate,
                kwargs=kwargs)
        self.gen_thread.start()

        for token in itertools.chain([f"## Generated by {self.model_name} model\n"], self.streamer):

            token = token.replace("<|endoftext|>", "")
            token = token.replace("<|end_of_text|>", "")


            self.last_response = self.last_response + token
            token = self.replace_spaces(token)
            token = token.replace("\n", "  \n")
            self.cache[input_text] = self.cache[input_text] + token
            yield token